# Server (UI API) — backend runs on this port (8002 chosen to avoid conflict with Docker on 8000/8001).
PORT=8002

# Frontend: set in .env so Vite exposes it. If set, the app calls the backend at this URL
# (e.g. http://localhost:8002) instead of using the proxy. Run backend first, then npm run dev:client.
# VITE_API_URL=http://localhost:8002

# Temporal server (gRPC); task queue must match Worker QUEUE_NAME (e.g. core-task-queue)
TEMPORAL_ADDRESS=localhost:7233
TEMPORAL_NAMESPACE=default
TEMPORAL_TASK_QUEUE=core-task-queue

# Set to 1 for stub LLM replies when Temporal is unavailable
USE_STUB_LLM=0

# Chat tab pipelines. Format: id:Label,id2:Label2 (comma-separated). Unset/empty = use built-in defaults.
PIPELINE_OPTIONS=llama-oss:Llama OSS,openai-oss:OpenAI OSS,both:Both models,chat-mistral:Mistral,chat-llama3.2:Llama 3.2,chat-phi3:Phi-3,chat-gemma2-2b:Gemma 2 2B,chat-qwen2-1.5b:Qwen2 1.5B,query-all-models:Query all models
# RAG tab pipelines. Format: id:Label,id2:Label2 (comma-separated). Unset/empty = use built-in defaults.
PIPELINE_OPTIONS_RAG=question-answer:Question-Answer (RAG),rag-llama-oss:RAG Llama OSS,rag-openai-oss:RAG OpenAI OSS,rag-both:RAG Both models,rag-mistral:RAG Mistral,rag-llama3.2:RAG Llama 3.2,rag-phi3:RAG Phi-3,rag-gemma2-2b:RAG Gemma 2 2B,rag-qwen2-1.5b:RAG Qwen2 1.5B

# Mounted folder for payload templates: <pipeline>_<kind>.tpl (e.g. llama-oss_chat.tpl)
TEMPLATES_DIR=/mnt/templates

# Chat: workflow started only when user sends a message (payload from template, wait for response)
# Must match Worker's registered workflow type (e.g. CoreWorkflow)
TEMPORAL_CHAT_WORKFLOW=CoreWorkflow
TEMPORAL_WORKFLOW_ID_TEMPLATE=chat-{{pipelineId}}-{{timestamp}}
TEMPORAL_WORKFLOW_CLASS=core-task-queue
# Max ms to wait for workflow result (default 120000). If no Worker runs the workflow, API returns this error.
# TEMPORAL_CHAT_RESULT_TIMEOUT_MS=120000

# RAG upload (doc ingestion) workflow — name, ID template, task queue (same core workflow)
# TEMPORAL_DOC_WORKFLOW=CoreWorkflow
# TEMPORAL_DOC_WORKFLOW_ID_TEMPLATE=doc-ingest-{{ragTag}}-{{timestamp}}
# TEMPORAL_DOC_TASK_QUEUE=core-task-queue

# Optional: persist conversations/messages to file when Redis is not used (e.g. data/store.json)
# STORE_FILE=data/store.json

# Redis for UI store: two buckets olo-ui:chat (Chat tab) and olo-ui:rag (RAG tab).
# If set, file store is ignored; delete conversation removes from Redis as well.
# REDIS_URL=redis://localhost:6379
# Or:
# REDIS_HOST=localhost
# REDIS_PORT=6379
# REDIS_PASSWORD=
